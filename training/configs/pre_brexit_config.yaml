# Pre-Brexit Period Training Configuration (2013-2016)
# Inherits from base_config.yaml with period-specific overrides

# Model Configuration
model:
  name: "meta-llama/Llama-2-7b-hf"
  cache_dir: "./models/cache"
  torch_dtype: "bfloat16"
  attn_implementation: "flash_attention_2"

# Training Configuration  
training:
  output_dir: "./models/pre_brexit_2013_2016"
  run_name: "llama_legal_pre_brexit_2013_2016"
  
  # Adjusted for dataset size (35.3M tokens, 22,144 chunks)
  learning_rate: 1e-5  # Slightly lower for continued pretraining
  num_train_epochs: 2
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 4
  
  max_seq_length: 2048
  save_steps: 250  # More frequent saves for smaller dataset
  eval_steps: 250
  
  bf16: true
  gradient_checkpointing: true

# Data Configuration
data:
  data_dir: "../preprocessing/outputs/llama_training_ready"
  dataset_name: "pre_brexit_2013_2016"
  train_split: 0.95
  eval_split: 0.05
  seed: 42
  text_column: "text"
  add_eos_token: true

# Logging
logging:
  wandb_project: "llama-legal-pretraining"
  wandb_tags: ["pre-brexit", "2013-2016", "legal", "tribunal"]
  
# Evaluation
evaluation:
  eval_dataset_size: 500
  compute_perplexity: true
  generate_samples: true
  generation_max_length: 256
  generation_num_samples: 3 