# Pre-Brexit (2013-2016) LLaMA Fine-tuning Configuration - Speed Optimized
model:
  model_name_or_path: "meta-llama/Meta-Llama-3-8B"  # Restored 8B model
  use_flash_attention_2: false
  torch_dtype: "bfloat16"
  device_map: "cuda:3"  # Use GPU 3 (A100 with ~67GB available)
  use_cache: false  # Disable KV cache during training for memory efficiency
  low_cpu_mem_usage: true  # Load model with minimal CPU memory usage

data:
  data_dir: "/data/shil6369/gov_scraper/preprocessing/outputs/llama_training_ready/pre_brexit_2013_2016"
  dataset_patterns: ["pre_brexit_2013_2016_chunks.jsonl"]
  train_split: 0.90
  eval_split: 0.10
  seed: 42  # Training randomness seed (can be overridden for robustness testing)
  data_split_seed: 42  # Fixed seed for train/test split (keep identical across rounds)
  text_column: "text"
  add_eos_token: true
  chunk_size: 512  # Further reduced for speed
  preprocessing_num_workers: 4  # Increased for faster data loading
  # Full dataset - no sample limits

training:
  output_dir: "models/llama3_8b_pre_brexit"
  run_name: "llama3_8b_pre_brexit_2013_2016"
  
  # Speed-optimized parameters
  learning_rate: 3e-5  # Higher LR to compensate for fewer steps
  num_train_epochs: 2  # Reduced epochs for faster completion
  per_device_train_batch_size: 2  # Slightly larger batch
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 16  # Reduced accumulation for faster steps
  dataloader_num_workers: 4  # Parallel data loading
  dataloader_pin_memory: true  # Faster GPU transfer
  
  # Training stability
  warmup_ratio: 0.05
  weight_decay: 0.01
  max_grad_norm: 1.0
  lr_scheduler_type: "cosine"
  
  # Less frequent checkpointing for speed
  save_strategy: "steps"
  save_steps: 5000  # Less frequent saves
  save_total_limit: 1
  save_only_model: true
  
  # Less frequent evaluation for speed
  evaluation_strategy: "steps"
  eval_steps: 5000  # Less frequent evaluation
  max_eval_samples: 50  # Minimal eval
  
  # Logging
  logging_steps: 50  # More frequent logging to monitor progress
  log_level: "info"
  
  # Speed optimizations
  use_deepspeed: false
  use_wandb: false
  fp16: false
  bf16: true
  remove_unused_columns: true  # Remove unused data columns
  dataloader_drop_last: true   # Avoid partial batches 